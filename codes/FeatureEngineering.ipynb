{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b520eec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import os\n",
    "import fasttext.util\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dc32408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/training_data_ra_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd7a0cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'pubdatetime', 'publication_name', 'title_h1', 'text_200',\n",
       "       'relevant', 'solution_frame', 'problem_frame', 'title_h2',\n",
       "       'articleHead', 'text', 'text_lead', 'text_body', 'publication_type',\n",
       "       'publication_edition', 'wordCount', 'country', 'state', 'city',\n",
       "       'matches', 'matches_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13347877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                     0.000000\n",
       "pubdatetime            0.000000\n",
       "publication_name       0.005036\n",
       "title_h1               0.004443\n",
       "text_200               0.000000\n",
       "relevant               0.000000\n",
       "solution_frame         0.000000\n",
       "problem_frame          0.000000\n",
       "title_h2               0.939870\n",
       "articleHead            0.534656\n",
       "text                   0.267773\n",
       "text_lead              0.299171\n",
       "text_body              0.267773\n",
       "publication_type       0.267773\n",
       "publication_edition    0.869964\n",
       "wordCount              0.267773\n",
       "country                0.390995\n",
       "state                  0.578791\n",
       "city                   0.678910\n",
       "matches                0.267773\n",
       "matches_count          0.267773\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e44a2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"title_h2\", \"articleHead\", \"publication_edition\", \"solution_frame\",\\\n",
    "         \"problem_frame\", \"state\", \"city\", \"matches\", \"matches_count\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd699495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pubdatetime</th>\n",
       "      <th>publication_name</th>\n",
       "      <th>title_h1</th>\n",
       "      <th>text_200</th>\n",
       "      <th>relevant</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lead</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publication_type</th>\n",
       "      <th>wordCount</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804124</td>\n",
       "      <td>2021-05-10T00:00:00Z</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>The Hill's 12:30 Report - Presented by Faceboo...</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>Presented by Facebook  To view past editions o...</td>\n",
       "      <td>Web Publication;WebLinks</td>\n",
       "      <td>2031.0</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>554905</td>\n",
       "      <td>2020-10-30T00:00:00Z</td>\n",
       "      <td>The Arizona Republic (Phoenix)</td>\n",
       "      <td>'60 Minutes' interviews stark study in contrasts</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>This is not a political observation. It's a te...</td>\n",
       "      <td>Newspaper;Newspapers</td>\n",
       "      <td>734.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>798375</td>\n",
       "      <td>2021-01-05T00:00:00Z</td>\n",
       "      <td>Newstex Blogs</td>\n",
       "      <td>Is There a Case for Principled Populism From t...</td>\n",
       "      <td>\\r\\n\\r\\nJan 05, 2021( Conservative Daily News:...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nJan 05, 2021( Conservative Daily News: htt...</td>\n",
       "      <td>Jan 05, 2021( Conservative Daily News:</td>\n",
       "      <td>In the past 10 years, we have seen a new entra...</td>\n",
       "      <td>Web Blog;Blogs</td>\n",
       "      <td>1689.0</td>\n",
       "      <td>UNITED STATES / CHINA / EUROPE / HUNGARY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>691445</td>\n",
       "      <td>2020-11-10T00:00:00Z</td>\n",
       "      <td>Canadian Press</td>\n",
       "      <td>Election breathes new life into false 'dead vo...</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>0</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>The false claim that deceased voters cast vote...</td>\n",
       "      <td>Newswire;Newswires &amp; Press Releases</td>\n",
       "      <td>886.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>490404</td>\n",
       "      <td>2020-10-07T00:00:00Z</td>\n",
       "      <td>Tampa Bay Times</td>\n",
       "      <td>Viewer's Guide: Virus response on stage with P...</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>The 90-minute debate will be divided into nine...</td>\n",
       "      <td>Newspaper;Newspapers</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>UNITED STATES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           pubdatetime                publication_name  \\\n",
       "0  804124  2021-05-10T00:00:00Z                        The Hill   \n",
       "1  554905  2020-10-30T00:00:00Z  The Arizona Republic (Phoenix)   \n",
       "2  798375  2021-01-05T00:00:00Z                   Newstex Blogs   \n",
       "3  691445  2020-11-10T00:00:00Z                  Canadian Press   \n",
       "4  490404  2020-10-07T00:00:00Z                 Tampa Bay Times   \n",
       "\n",
       "                                            title_h1  \\\n",
       "0  The Hill's 12:30 Report - Presented by Faceboo...   \n",
       "1   '60 Minutes' interviews stark study in contrasts   \n",
       "2  Is There a Case for Principled Populism From t...   \n",
       "3  Election breathes new life into false 'dead vo...   \n",
       "4  Viewer's Guide: Virus response on stage with P...   \n",
       "\n",
       "                                            text_200  relevant  \\\n",
       "0  Presented by Facebook To view past editions of...         0   \n",
       "1  Having seen both the unedited footage of Presi...         0   \n",
       "2  \\r\\n\\r\\nJan 05, 2021( Conservative Daily News:...         0   \n",
       "3  As President Donald Trump continued to assert ...         0   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...         0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Presented by Facebook To view past editions of...   \n",
       "1  Having seen both the unedited footage of Presi...   \n",
       "2  \\n\\nJan 05, 2021( Conservative Daily News: htt...   \n",
       "3  As President Donald Trump continued to assert ...   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...   \n",
       "\n",
       "                                           text_lead  \\\n",
       "0  Presented by Facebook To view past editions of...   \n",
       "1  Having seen both the unedited footage of Presi...   \n",
       "2             Jan 05, 2021( Conservative Daily News:   \n",
       "3  As President Donald Trump continued to assert ...   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...   \n",
       "\n",
       "                                           text_body  \\\n",
       "0  Presented by Facebook  To view past editions o...   \n",
       "1  This is not a political observation. It's a te...   \n",
       "2  In the past 10 years, we have seen a new entra...   \n",
       "3  The false claim that deceased voters cast vote...   \n",
       "4  The 90-minute debate will be divided into nine...   \n",
       "\n",
       "                      publication_type  wordCount  \\\n",
       "0             Web Publication;WebLinks     2031.0   \n",
       "1                 Newspaper;Newspapers      734.0   \n",
       "2                       Web Blog;Blogs     1689.0   \n",
       "3  Newswire;Newswires & Press Releases      886.0   \n",
       "4                 Newspaper;Newspapers     1062.0   \n",
       "\n",
       "                                    country  \n",
       "0                             UNITED STATES  \n",
       "1                                       NaN  \n",
       "2  UNITED STATES / CHINA / EUROPE / HUNGARY  \n",
       "3                                       NaN  \n",
       "4                             UNITED STATES  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "840b7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.text.notna()].isna().sum() / len(df[df.text.notna()])\n",
    "df = df[df.text.notna()]\n",
    "df = df[df.relevant != 99]\n",
    "df = df[df.title_h1.notna()]\n",
    "df = df[df.text_lead.notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "320ae8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.preprocessing import Preprocessor\n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0368a473",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:00<00:00, 3672.49it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:01<00:00, 1529.22it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:18<00:00, 126.86it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:05<00:00, 439.10it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:01<00:00, 1463.55it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 2354/2354 [00:00<00:00, 4809.75it/s]\n"
     ]
    }
   ],
   "source": [
    "df = preprocessor.perform_clean_lemmatize_tokenize(df, \"title_h1\")\n",
    "df = preprocessor.perform_clean_lemmatize_tokenize(df, \"text\")\n",
    "df = preprocessor.perform_clean_lemmatize_tokenize(df, \"text_lead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0ae9106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pubdatetime</th>\n",
       "      <th>publication_name</th>\n",
       "      <th>title_h1</th>\n",
       "      <th>text_200</th>\n",
       "      <th>relevant</th>\n",
       "      <th>text</th>\n",
       "      <th>text_lead</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publication_type</th>\n",
       "      <th>...</th>\n",
       "      <th>country</th>\n",
       "      <th>cleaned_title_h1</th>\n",
       "      <th>lemmatized_title_h1</th>\n",
       "      <th>tokens_title_h1</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>tokens_text</th>\n",
       "      <th>cleaned_text_lead</th>\n",
       "      <th>lemmatized_text_lead</th>\n",
       "      <th>tokens_text_lead</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>804124</td>\n",
       "      <td>2021-05-10T00:00:00Z</td>\n",
       "      <td>The Hill</td>\n",
       "      <td>The Hill's 12:30 Report - Presented by Faceboo...</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>0</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>Presented by Facebook To view past editions of...</td>\n",
       "      <td>Presented by Facebook  To view past editions o...</td>\n",
       "      <td>Web Publication;WebLinks</td>\n",
       "      <td>...</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>hills report presented facebook biden reverses...</td>\n",
       "      <td>hill report present facebook biden reverse tru...</td>\n",
       "      <td>[hill, report, present, facebook, biden, rever...</td>\n",
       "      <td>presented facebook view past editions hills re...</td>\n",
       "      <td>present facebook view past editions hill repor...</td>\n",
       "      <td>[present, facebook, view, past, editions, hill...</td>\n",
       "      <td>presented facebook view past editions hills re...</td>\n",
       "      <td>present facebook view past editions hill repor...</td>\n",
       "      <td>[present, facebook, view, past, editions, hill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>554905</td>\n",
       "      <td>2020-10-30T00:00:00Z</td>\n",
       "      <td>The Arizona Republic (Phoenix)</td>\n",
       "      <td>'60 Minutes' interviews stark study in contrasts</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>0</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>Having seen both the unedited footage of Presi...</td>\n",
       "      <td>This is not a political observation. It's a te...</td>\n",
       "      <td>Newspaper;Newspapers</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>minutes interviews stark study contrasts</td>\n",
       "      <td>minutes interview stark study contrast</td>\n",
       "      <td>[minutes, interview, stark, study, contrast]</td>\n",
       "      <td>seen unedited footage president donald trumps ...</td>\n",
       "      <td>see unedited footage president donald trump in...</td>\n",
       "      <td>[see, unedited, footage, president, donald, tr...</td>\n",
       "      <td>seen unedited footage president donald trumps ...</td>\n",
       "      <td>see unedited footage president donald trump in...</td>\n",
       "      <td>[see, unedited, footage, president, donald, tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>798375</td>\n",
       "      <td>2021-01-05T00:00:00Z</td>\n",
       "      <td>Newstex Blogs</td>\n",
       "      <td>Is There a Case for Principled Populism From t...</td>\n",
       "      <td>\\r\\n\\r\\nJan 05, 2021( Conservative Daily News:...</td>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\nJan 05, 2021( Conservative Daily News: htt...</td>\n",
       "      <td>Jan 05, 2021( Conservative Daily News:</td>\n",
       "      <td>In the past 10 years, we have seen a new entra...</td>\n",
       "      <td>Web Blog;Blogs</td>\n",
       "      <td>...</td>\n",
       "      <td>UNITED STATES / CHINA / EUROPE / HUNGARY</td>\n",
       "      <td>case principled populism gop</td>\n",
       "      <td>case principled populism gop</td>\n",
       "      <td>[case, principled, populism, gop]</td>\n",
       "      <td>jan conservative daily news delivered newstex ...</td>\n",
       "      <td>jan conservative daily news deliver newstex po...</td>\n",
       "      <td>[jan, conservative, daily, news, deliver, news...</td>\n",
       "      <td>jan conservative daily news</td>\n",
       "      <td>jan conservative daily news</td>\n",
       "      <td>[jan, conservative, daily, news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>691445</td>\n",
       "      <td>2020-11-10T00:00:00Z</td>\n",
       "      <td>Canadian Press</td>\n",
       "      <td>Election breathes new life into false 'dead vo...</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>0</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>As President Donald Trump continued to assert ...</td>\n",
       "      <td>The false claim that deceased voters cast vote...</td>\n",
       "      <td>Newswire;Newswires &amp; Press Releases</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>election breathes new life false dead voter cl...</td>\n",
       "      <td>election breathe new life false dead voter claim</td>\n",
       "      <td>[election, breathe, new, life, false, dead, vo...</td>\n",
       "      <td>president donald trump continued assert withou...</td>\n",
       "      <td>president donald trump continue assert without...</td>\n",
       "      <td>[president, donald, trump, continue, assert, w...</td>\n",
       "      <td>president donald trump continued assert withou...</td>\n",
       "      <td>president donald trump continue assert without...</td>\n",
       "      <td>[president, donald, trump, continue, assert, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>490404</td>\n",
       "      <td>2020-10-07T00:00:00Z</td>\n",
       "      <td>Tampa Bay Times</td>\n",
       "      <td>Viewer's Guide: Virus response on stage with P...</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>0</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>Mike Pence and Kamala Harris do not have a tou...</td>\n",
       "      <td>The 90-minute debate will be divided into nine...</td>\n",
       "      <td>Newspaper;Newspapers</td>\n",
       "      <td>...</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>viewers guide virus response stage pence harris</td>\n",
       "      <td>viewers guide virus response stage pence harris</td>\n",
       "      <td>[viewers, guide, virus, response, stage, pence...</td>\n",
       "      <td>mike pence kamala harris tough act followthe v...</td>\n",
       "      <td>mike pence kamala harris tough act followthe v...</td>\n",
       "      <td>[mike, pence, kamala, harris, tough, act, foll...</td>\n",
       "      <td>mike pence kamala harris tough act followthe v...</td>\n",
       "      <td>mike pence kamala harris tough act followthe v...</td>\n",
       "      <td>[mike, pence, kamala, harris, tough, act, foll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id           pubdatetime                publication_name  \\\n",
       "0  804124  2021-05-10T00:00:00Z                        The Hill   \n",
       "1  554905  2020-10-30T00:00:00Z  The Arizona Republic (Phoenix)   \n",
       "2  798375  2021-01-05T00:00:00Z                   Newstex Blogs   \n",
       "3  691445  2020-11-10T00:00:00Z                  Canadian Press   \n",
       "4  490404  2020-10-07T00:00:00Z                 Tampa Bay Times   \n",
       "\n",
       "                                            title_h1  \\\n",
       "0  The Hill's 12:30 Report - Presented by Faceboo...   \n",
       "1   '60 Minutes' interviews stark study in contrasts   \n",
       "2  Is There a Case for Principled Populism From t...   \n",
       "3  Election breathes new life into false 'dead vo...   \n",
       "4  Viewer's Guide: Virus response on stage with P...   \n",
       "\n",
       "                                            text_200  relevant  \\\n",
       "0  Presented by Facebook To view past editions of...         0   \n",
       "1  Having seen both the unedited footage of Presi...         0   \n",
       "2  \\r\\n\\r\\nJan 05, 2021( Conservative Daily News:...         0   \n",
       "3  As President Donald Trump continued to assert ...         0   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...         0   \n",
       "\n",
       "                                                text  \\\n",
       "0  Presented by Facebook To view past editions of...   \n",
       "1  Having seen both the unedited footage of Presi...   \n",
       "2  \\n\\nJan 05, 2021( Conservative Daily News: htt...   \n",
       "3  As President Donald Trump continued to assert ...   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...   \n",
       "\n",
       "                                           text_lead  \\\n",
       "0  Presented by Facebook To view past editions of...   \n",
       "1  Having seen both the unedited footage of Presi...   \n",
       "2             Jan 05, 2021( Conservative Daily News:   \n",
       "3  As President Donald Trump continued to assert ...   \n",
       "4  Mike Pence and Kamala Harris do not have a tou...   \n",
       "\n",
       "                                           text_body  \\\n",
       "0  Presented by Facebook  To view past editions o...   \n",
       "1  This is not a political observation. It's a te...   \n",
       "2  In the past 10 years, we have seen a new entra...   \n",
       "3  The false claim that deceased voters cast vote...   \n",
       "4  The 90-minute debate will be divided into nine...   \n",
       "\n",
       "                      publication_type  ...  \\\n",
       "0             Web Publication;WebLinks  ...   \n",
       "1                 Newspaper;Newspapers  ...   \n",
       "2                       Web Blog;Blogs  ...   \n",
       "3  Newswire;Newswires & Press Releases  ...   \n",
       "4                 Newspaper;Newspapers  ...   \n",
       "\n",
       "                                    country  \\\n",
       "0                             UNITED STATES   \n",
       "1                                       NaN   \n",
       "2  UNITED STATES / CHINA / EUROPE / HUNGARY   \n",
       "3                                       NaN   \n",
       "4                             UNITED STATES   \n",
       "\n",
       "                                    cleaned_title_h1  \\\n",
       "0  hills report presented facebook biden reverses...   \n",
       "1           minutes interviews stark study contrasts   \n",
       "2                       case principled populism gop   \n",
       "3  election breathes new life false dead voter cl...   \n",
       "4    viewers guide virus response stage pence harris   \n",
       "\n",
       "                                 lemmatized_title_h1  \\\n",
       "0  hill report present facebook biden reverse tru...   \n",
       "1             minutes interview stark study contrast   \n",
       "2                       case principled populism gop   \n",
       "3   election breathe new life false dead voter claim   \n",
       "4    viewers guide virus response stage pence harris   \n",
       "\n",
       "                                     tokens_title_h1  \\\n",
       "0  [hill, report, present, facebook, biden, rever...   \n",
       "1       [minutes, interview, stark, study, contrast]   \n",
       "2                  [case, principled, populism, gop]   \n",
       "3  [election, breathe, new, life, false, dead, vo...   \n",
       "4  [viewers, guide, virus, response, stage, pence...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  presented facebook view past editions hills re...   \n",
       "1  seen unedited footage president donald trumps ...   \n",
       "2  jan conservative daily news delivered newstex ...   \n",
       "3  president donald trump continued assert withou...   \n",
       "4  mike pence kamala harris tough act followthe v...   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  present facebook view past editions hill repor...   \n",
       "1  see unedited footage president donald trump in...   \n",
       "2  jan conservative daily news deliver newstex po...   \n",
       "3  president donald trump continue assert without...   \n",
       "4  mike pence kamala harris tough act followthe v...   \n",
       "\n",
       "                                         tokens_text  \\\n",
       "0  [present, facebook, view, past, editions, hill...   \n",
       "1  [see, unedited, footage, president, donald, tr...   \n",
       "2  [jan, conservative, daily, news, deliver, news...   \n",
       "3  [president, donald, trump, continue, assert, w...   \n",
       "4  [mike, pence, kamala, harris, tough, act, foll...   \n",
       "\n",
       "                                   cleaned_text_lead  \\\n",
       "0  presented facebook view past editions hills re...   \n",
       "1  seen unedited footage president donald trumps ...   \n",
       "2                        jan conservative daily news   \n",
       "3  president donald trump continued assert withou...   \n",
       "4  mike pence kamala harris tough act followthe v...   \n",
       "\n",
       "                                lemmatized_text_lead  \\\n",
       "0  present facebook view past editions hill repor...   \n",
       "1  see unedited footage president donald trump in...   \n",
       "2                        jan conservative daily news   \n",
       "3  president donald trump continue assert without...   \n",
       "4  mike pence kamala harris tough act followthe v...   \n",
       "\n",
       "                                    tokens_text_lead  \n",
       "0  [present, facebook, view, past, editions, hill...  \n",
       "1  [see, unedited, footage, president, donald, tr...  \n",
       "2                   [jan, conservative, daily, news]  \n",
       "3  [president, donald, trump, continue, assert, w...  \n",
       "4  [mike, pence, kamala, harris, tough, act, foll...  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7564c7ed",
   "metadata": {},
   "source": [
    "### ----------------------------- train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "584ddeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.25, shuffle=True, random_state=234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec4499a",
   "metadata": {},
   "source": [
    "### ----------------------------- Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5ea7e2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(data: pd.Series, y: pd.Series, mode=\"all\"):\n",
    "    if mode==\"all\":\n",
    "        raw = data[y==1]\n",
    "        lens_dist = raw.apply(lambda x: len(x.split()))\n",
    "        num_new = sum(y==0) - sum(y==1)\n",
    "        words = raw.explode()\n",
    "        new_data = pd.Series([\" \".join(np.random.choice(words, size=int(np.random.choice(lens_dist)))) for _ in range(num_new)])\n",
    "        new_y = pd.Series([1 for _ in range(num_new)])\n",
    "        data_all = pd.concat([data, new_data])\n",
    "        y_all = pd.concat([y, new_y])\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df[\"data\"] = data_all\n",
    "        temp_df[\"y\"] = y_all\n",
    "        temp_df = temp_df.sample(frac=1)\n",
    "        return temp_df.data, temp_df.y\n",
    "    \n",
    "    if mode==\"most\":\n",
    "        raw = data[y==1]\n",
    "        lens_dist = raw.apply(lambda x: len(x.split()))\n",
    "        num_new = sum(y==0) - sum(y==1)\n",
    "        words = raw.apply(lambda x: x.split()).explode().value_counts()[:20]\n",
    "        \n",
    "        def get_new_title():\n",
    "            tokens = [random.choices(list(words.index), weights=list(words))[0] for _ in range(int(np.random.choice(lens_dist)))]\n",
    "            return \" \".join(tokens)\n",
    "        \n",
    "        new_data = pd.Series([get_new_title() for _ in range(num_new)])\n",
    "        new_y = pd.Series([1 for _ in range(num_new)])\n",
    "        data_all = pd.concat([data, new_data])\n",
    "        y_all = pd.concat([y, new_y])\n",
    "        temp_df = pd.DataFrame()\n",
    "        temp_df[\"data\"] = data_all\n",
    "        temp_df[\"y\"] = y_all\n",
    "        temp_df = temp_df.sample(frac=1)\n",
    "        return temp_df.data, temp_df.y\n",
    "        \n",
    "    raise Exception(\"mode not specified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb4388",
   "metadata": {},
   "source": [
    "### ----------------------------- Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bf6c3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_whole_text(df, data_augmentation=True):\n",
    "    data, y = df.tokens_text.apply(lambda x: \" \".join(x)), df.relevant\n",
    "    if data_augmentation:\n",
    "        data, y = augment_data(data, y)\n",
    "        return data, y\n",
    "    return data, y\n",
    "\n",
    "def get_title(df, is_final_feature, data_augmentation=True):\n",
    "    if is_final_feature:\n",
    "        temp_df = df[[\"tokens_title_h1\", \"relevant\"]]\n",
    "        temp_df[\"titles\"] = df.tokens_title_h1.apply(lambda x: \" \".join(x))\n",
    "        temp_df = temp_df[temp_df.titles.notna()]\n",
    "        temp_df = temp_df[temp_df.titles.apply(len) > 2]\n",
    "        data, y = temp_df.titles, temp_df.relevant\n",
    "        if data_augmentation:\n",
    "            data, y = augment_data(data, y, mode=\"most\")\n",
    "            return data, y\n",
    "        return data, y ## indexes rearenged\n",
    "        \n",
    "    else: \n",
    "        titles = df.tokens_title_h1.apply(lambda x: \" \".join(x))\n",
    "        return titles, df.relevant  ## original indexes\n",
    "    \n",
    "def get_title_plus_first_paragraph(df, data_augmentation=True):\n",
    "    title, y = get_title(df, is_final_feature=False)\n",
    "    title = title.apply(lambda x: f\"{x} \")\n",
    "    \n",
    "    text_lead = df.tokens_text_lead.apply(lambda x: \" \".join(x))\n",
    "    data, y = title.str.cat(text_lead), y\n",
    "    if data_augmentation:\n",
    "        data, y = augment_data(data, y)\n",
    "        return data, y\n",
    "    return data, y\n",
    "\n",
    "def get_x_first_sentence(text: str, num: int):\n",
    "    sentences = text.split(\".\")\n",
    "    if num == 1:\n",
    "        for sen in sentences:\n",
    "            if len(sen.split(\" \")) > 1:\n",
    "                return sen\n",
    "    else:\n",
    "        sens = []\n",
    "        for sen in sentences:\n",
    "            if len(sen.split(\" \")) > 1:\n",
    "                sens.append(sen)\n",
    "                if len(sens) == num:\n",
    "                    return sens\n",
    "        return sens\n",
    "\n",
    "def get_title_plus_first_sentence(df, data_augmentation=True):\n",
    "    title, y = get_title(df, is_final_feature=False)\n",
    "    title = title.apply(lambda x: f\"{x} \")\n",
    "    first_sentences = df.text.apply(lambda x: get_x_first_sentence(x, num=1))\n",
    "    first_sentences = first_sentences.apply(lambda x: preprocessor.clean_query(x)).apply(lambda x: \" \".join(x))\n",
    "    data, y = title.str.cat(first_sentences), y\n",
    "    if data_augmentation:\n",
    "        data, y = augment_data(data, y)\n",
    "        return data, y\n",
    "    return data, y\n",
    "\n",
    "def get_title_plus_x_sentences(df, num_sentences, data_augmentation=True):\n",
    "    title, y = get_title(df, is_final_feature=False)\n",
    "    title = title.apply(lambda x: f\"{x} \")\n",
    "    sentences = df.text.apply(lambda x: get_x_first_sentence(x, num=num_sentences))\n",
    "    sentences = sentences.apply(lambda x: \" \".join(x))\n",
    "    sentences = sentences.apply(lambda x: preprocessor.clean_query(x)).apply(lambda x: \" \".join(x))\n",
    "    data, y = title.str.cat(sentences), y\n",
    "    if data_augmentation:\n",
    "        data, y = augment_data(data, y)\n",
    "        return data, y\n",
    "    return data, y\n",
    "    \n",
    "def get_text_paragraphs(text: str):\n",
    "    initial_paragraphs = text.split(\"\\n\")\n",
    "    final_paragraphs = []\n",
    "    for par in initial_paragraphs:\n",
    "        if len(par.split(\" \")) > 3:\n",
    "            final_paragraphs.append(par)\n",
    "    return final_paragraphs\n",
    "\n",
    "def get_title_plus_first_sentence_each_paragraph(df, data_augmentation=True):\n",
    "    title, y = get_title(df, is_final_feature=False)\n",
    "    title = title.apply(lambda x: f\"{x} \")\n",
    "    \n",
    "    paragraphs = df.text.apply(get_text_paragraphs)\n",
    "    paragraphs_first_sentences = paragraphs.apply(lambda x: [get_x_first_sentence(par, num=1) for par in x])\n",
    "    paragraphs_first_sentences = paragraphs_first_sentences.apply(lambda x: \" \".join(x))\n",
    "    paragraphs_first_sentences = paragraphs_first_sentences.apply(lambda x: preprocessor.clean_query(x)).apply(lambda x: \" \".join(x))\n",
    "    data, y = title.str.cat(paragraphs_first_sentences), y\n",
    "    if data_augmentation:\n",
    "        data, y = augment_data(data, y)\n",
    "        return data, y\n",
    "    return data, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2912075",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb3a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fd230fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_models = [GradientBoostingClassifier, svm.SVC, LogisticRegression, RandomForestClassifier]\n",
    "features = [\n",
    "            {\"name\": \"title\", \"function\": get_title},\n",
    "            {\"name\": \"title_plus_first_paragraph\", \"function\": get_title_plus_first_paragraph},\n",
    "            {\"name\": \"title_plus_first_sentence\", \"function\": get_title_plus_first_sentence},\n",
    "            {\"name\": \"title_plus_5_sentences\", \"function\": get_title_plus_x_sentences},\n",
    "            {\"name\": \"title_plus_10_sentences\", \"function\": get_title_plus_x_sentences},\n",
    "            \n",
    "            {\"name\": \"title_plus_first_sentence_each_paragraph\",\n",
    "             \"function\": get_title_plus_first_sentence_each_paragraph},\n",
    "            {\"name\": \"whole_text\", \"function\": get_whole_text}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e47f3",
   "metadata": {},
   "source": [
    "### ----------------------------- Embedding with all-MiniLM-L6-v2 model - Classification with classic algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a16e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = SentenceTransformer('./transformer-model')\n",
    "\n",
    "def embed(data: pd.Series):\n",
    "    embeddings = data.apply(lambda x: emb_model.encode(str(x)))\n",
    "    embeddings_arr = np.zeros((len(data), 384))\n",
    "    for row, emb in enumerate(embeddings):\n",
    "        embeddings_arr[row, :] = emb\n",
    "        \n",
    "    return embeddings_arr\n",
    "\n",
    "def train_model(model, training_data, y_train):\n",
    "    model = model.fit(training_data, y_train)\n",
    "    return model\n",
    "\n",
    "def test_model(model, test_data, y_test):\n",
    "    y_pred = model.predict(test_data)\n",
    "    return classification_report(y_test, y_pred)\n",
    "\n",
    "def get_data(feature_combination_name: str, function, df, data_augmentation: bool):\n",
    "    if feature_combination_name == \"title_plus_5_sentences\":\n",
    "        X, y = function(df, 5, data_augmentation)\n",
    "    elif feature_combination_name == \"title_plus_10_sentences\":\n",
    "        X, y = function(df, 10, data_augmentation)\n",
    "    elif feature_combination_name == \"title\":\n",
    "        X, y = function(df, is_final_feature=True, data_augmentation=data_augmentation)\n",
    "    else:\n",
    "        X, y = function(df, data_augmentation)\n",
    "        \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fc63fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title\n",
      "True\n",
      "3\n",
      "(3160,) (3160,)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for f_dict in features:\n",
    "    name = f_dict.get(\"name\")\n",
    "    print(name)\n",
    "    func = f_dict.get(\"function\")\n",
    "    data, y = get_data(name, func, df_train, data_augmentation=True)\n",
    "    print(all(data.apply(type) == str))\n",
    "    print(data.apply(len).min())\n",
    "    print(data.shape, y.shape)\n",
    "    print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab7e978",
   "metadata": {},
   "source": [
    "### ----------------------------- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7bbb9d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Feature:  title\n",
      "Data prep\n",
      "Embedding\n",
      "\tModel:  GradientBoostingClassifier() id:  140461155616944\n",
      "\t\t training\n",
      "\tModel:  SVC(class_weight='balanced') id:  140461154192784\n",
      "\t\t training\n",
      "\tModel:  LogisticRegression(class_weight='balanced') id:  140461124282400\n",
      "\t\t training\n",
      "\tModel:  RandomForestClassifier(class_weight='balanced') id:  140461124281200\n",
      "\t\t training\n"
     ]
    }
   ],
   "source": [
    "trained_models = []\n",
    "\n",
    "for feature in features:\n",
    "    print(\"-\" * 80)\n",
    "    feature_combination_name, function = feature[\"name\"], feature[\"function\"]\n",
    "    \n",
    "    print(\"Feature: \", feature_combination_name)\n",
    "    print(\"Data prep\")\n",
    "    \n",
    "    data, y = get_data(feature_combination_name, function, df_train, data_augmentation=True)\n",
    "    \n",
    "    print(\"Embedding\")\n",
    "    embedding = embed(data)\n",
    "    \n",
    "    for clf_model_class in classification_models:\n",
    "        \n",
    "        clf_model = clf_model_class()\n",
    "        clf_model.class_weight = \"balanced\"\n",
    "        \n",
    "        print(\"\\tModel: \", clf_model, \"id: \", id(clf_model))\n",
    "        print(\"\\t\\t training\")\n",
    "        clf_model = train_model(clf_model, embedding, y)\n",
    "        \n",
    "        trained_models.append({\n",
    "            \"feature_combination_name\": feature_combination_name,\n",
    "            \"model_name\": clf_model.__repr__(),\n",
    "            \"model\": clf_model\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189624b0",
   "metadata": {},
   "source": [
    "### ----------------------------- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7abfe8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "title\n",
      "GradientBoostingClassifier()\n",
      "140461155616944\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96       534\n",
      "           1       0.60      0.50      0.55        54\n",
      "\n",
      "    accuracy                           0.92       588\n",
      "   macro avg       0.78      0.73      0.75       588\n",
      "weighted avg       0.92      0.92      0.92       588\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "title\n",
      "SVC(class_weight='balanced')\n",
      "140461154192784\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       534\n",
      "           1       0.63      0.59      0.61        54\n",
      "\n",
      "    accuracy                           0.93       588\n",
      "   macro avg       0.79      0.78      0.79       588\n",
      "weighted avg       0.93      0.93      0.93       588\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "title\n",
      "LogisticRegression(class_weight='balanced')\n",
      "140461124282400\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       534\n",
      "           1       0.56      0.57      0.57        54\n",
      "\n",
      "    accuracy                           0.92       588\n",
      "   macro avg       0.76      0.76      0.76       588\n",
      "weighted avg       0.92      0.92      0.92       588\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "title\n",
      "RandomForestClassifier(class_weight='balanced')\n",
      "140461124281200\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.96       534\n",
      "           1       0.63      0.31      0.42        54\n",
      "\n",
      "    accuracy                           0.92       588\n",
      "   macro avg       0.78      0.65      0.69       588\n",
      "weighted avg       0.91      0.92      0.91       588\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "\n",
    "for f, feature in enumerate(features):\n",
    "    \n",
    "    feature_combination_name, function = feature[\"name\"], feature[\"function\"]\n",
    "    \n",
    "    data, y = get_data(feature_combination_name, function, df_test, data_augmentation=False)\n",
    "        \n",
    "    embedding = embed(data)\n",
    "    \n",
    "    for tm in trained_models:\n",
    "        \n",
    "        if tm.get(\"feature_combination_name\") == feature_combination_name:\n",
    "            \n",
    "            print(\"-\" * 80)\n",
    "            print(feature_combination_name)\n",
    "            print(tm.get(\"model_name\"))\n",
    "            \n",
    "            clf_model = tm.get(\"model\")\n",
    "            print(id(clf_model))\n",
    "            y_pred = clf_model.predict(embedding)\n",
    "            clf_report_dict = classification_report(y, y_pred, output_dict=True)\n",
    "            clf_report = classification_report(y, y_pred)\n",
    "            reports.append((tm.get(\"model_name\") + \" \" + feature_combination_name, clf_report_dict))\n",
    "\n",
    "            print()\n",
    "            print(clf_report)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d58f7f",
   "metadata": {},
   "source": [
    "### ------------------- Writing results to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "67688cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier() title_plus_first_paragraph\n",
      "SVC(class_weight='balanced') title_plus_first_paragraph\n",
      "LogisticRegression(class_weight='balanced') title_plus_first_paragraph\n",
      "RandomForestClassifier(class_weight='balanced') title_plus_first_paragraph\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "recs = []\n",
    "precs = []\n",
    "dff = pd.DataFrame()\n",
    "\n",
    "fe = \"title_plus_first_paragraph\"\n",
    "for name, report in reports:\n",
    "    \n",
    "    if name.split()[1]==fe:\n",
    "        print(name)\n",
    "        accs.append(round(report[\"accuracy\"], 2))\n",
    "        f1s.append(round(report[\"1\"][\"f1-score\"], 2))\n",
    "        recs.append(round(report[\"1\"][\"recall\"], 2))\n",
    "        precs.append(round(report[\"1\"][\"precision\"], 2))\n",
    "        \n",
    "dff[\"acc\"] = accs\n",
    "dff[\"f1\"] = f1s\n",
    "dff[\"recall\"] = recs\n",
    "dff[\"prec\"] = precs\n",
    "dff.to_excel(f\"{fe}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb3f54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1473fe46",
   "metadata": {},
   "source": [
    "# ----------------------------------- Manual Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb28dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbdf0a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, y = get_data(\"whole_text\", get_whole_text, df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fefb730d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = embed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d6d30689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1765, 384)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b7479018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf = RandomForestClassifier(class_weight=\"balanced\")\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_emb, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5cd99adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, y = get_data(\"whole_text\", get_whole_text, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "711ced24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_emb = embed(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26a9eca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      1.00      0.95       535\n",
      "           1       0.80      0.07      0.14        54\n",
      "\n",
      "    accuracy                           0.91       589\n",
      "   macro avg       0.86      0.54      0.55       589\n",
      "weighted avg       0.90      0.91      0.88       589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(test_emb)\n",
    "print(classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cac9096b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1694    washington us senate set vote early next week ...\n",
       "1679    jun international business time news deliver n...\n",
       "1131    people gather memorial day weekend port aransa...\n",
       "2206    oct better georgia newstex foreign interferenc...\n",
       "1147    accuse bombmaker identify ringleader plot kidn...\n",
       "                              ...                        \n",
       "732     dec wrap deliver newstex trump administration ...\n",
       "1551    washington ap joe bidens bet race simple one n...\n",
       "2372    full text charleston west virginia vice presid...\n",
       "1779    oct marketbeat deliver newstex combination pho...\n",
       "568     residents say counterprotesting pledge allegia...\n",
       "Name: tokens_text, Length: 589, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f01c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24     past years qanon movement baseless conspiracy ...\n",
       "222    people buy products incentivised leave positiv...\n",
       "377    new york may un human right office ohchr welco...\n",
       "508    interview cnn netflix ceo reveal decision remo...\n",
       "34     san francisco facebook incfacebook say wednesd...\n",
       "251    minister threaten hit social media giants fin ...\n",
       "113    googleowned youtube suspend former new york ci...\n",
       "122    canberra australia ap australian regulators ru...\n",
       "337    first congress override trump veto defense bil...\n",
       "229    new delhi june special cell delhi police regis...\n",
       "493    youtube reduce amount content spread conspirac...\n",
       "265    lisa laflamme federal government today vow inf...\n",
       "30     google earn tens millions pound every year all...\n",
       "389    effort reverse flood abuse platform twitter ro...\n",
       "507    tuesday us president insult top infectious dis...\n",
       "86     us election test facebook say chief executive ...\n",
       "46     may geller report deliver newstex gop ignore u...\n",
       "504    utah governor spencer cox sign legislation man...\n",
       "336    washington facebook chief executive mark zucke...\n",
       "81     washington nexstar days go election day senato...\n",
       "240    mar marketbeat deliver newstex combination pho...\n",
       "277    youtube already implement large number differe...\n",
       "521    rep ted deutch dflorida issue follow news rele...\n",
       "17     version article first appear reliable source n...\n",
       "135    feb voice america deliver newstex facebook wed...\n",
       "334    trump elaborate action could take threat trump...\n",
       "39     exmoderators tell guardian facebook underpay m...\n",
       "192    facebook plan spend fund research enrich under...\n",
       "67     nov newsbustersorg newstex twitter deliver pro...\n",
       "551    day suspend facebook another two years former ...\n",
       "339    recent spate disgust anonymous abuse online pl...\n",
       "66     jan geller report deliver newstex righteous ca...\n",
       "16     may wrap deliver newstex president donald trum...\n",
       "182    may pa pundit deliver newstex corinne weaver f...\n",
       "102    jan international business time news deliver n...\n",
       "396    latest national international news bbc speech ...\n",
       "388    twitter look overseas growth india stand faste...\n",
       "474    section create modern internet legislation cen...\n",
       "175    canadian company join grow list top internatio...\n",
       "378    jan bookworm room deli havered newstex theatri...\n",
       "384    new delhi jan union minister anurag thakur cat...\n",
       "283    major advertise publication cover facebook oft...\n",
       "33     wellington new zealand prime minister jacinda ...\n",
       "302    full text florida governor ron desantis announ...\n",
       "333    ever wonder exactly prominent qanon facebooks ...\n",
       "542    us president donald trump threaten wednesday s...\n",
       "202    jun benzinga deliver newstex amazon incs nasda...\n",
       "327    nspcc say plan defer criminal liability senior...\n",
       "213    new outside report find facebook allow group m...\n",
       "411    fume facebook delete post temper tantrum twitt...\n",
       "254    facebooks usfb share come increase pressure pa...\n",
       "121    washington today congressman sean casten il jo...\n",
       "112    dec inside search blog deliver newstex since o...\n",
       "433    social media journalists alike amplify preside...\n",
       "Name: tokens_text, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[y==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "021d1819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL TEXT\n",
      "\n",
      "Florida Governor Ron DeSantis announced this week that he would fine social media companies that ban political candidates. Every outlet from Fox News to MSNBC fired off missives about the bill. What got lost in the news coverage is that Silicon Valley deplatforms very few politicians, save shock-jocks like Donald Trump and Laura Loomer (if you want to call her a politician). The same cannot be said for sex workers.\n",
      "\n",
      "This month, Centro University released a study estimating that 46 percent of adult influencers reported losing access to Twitter or Instagram in the last year. The bans put a permanent dent in the stars’ income, with Centro estimating sex workers lose $260 million a year due to social media bans. You won’t hear DeSantis, Fox News, Glenn Greenwald, or any other so-called free speech warriors decrying porn stars’ lost incomes, so let me break down how social media companies are screwing over porn stars (and not screwing them in a good way!).\n",
      "\n",
      "Silicon Valley titans have revoked my social media access multiple times. Take my recent Snapchat ban. The Santa Monica-based app barred me from posting on my public account, so I lost the means to communicate with fans who would message me on Snap. I lost 50 percent of my revenue until I built back my following.\n",
      "\n",
      "Other adult performers have faced far worse. The Centro report shows that 39.7 percent of adult influencers report temporarily losing Instagram accounts, 15.3 percent say they have been briefly removed from Twitter, and 8.7 percent claim to have been suspended from both. Many performers regain access to their accounts, but not everyone. Approximately 10.5 percent of performers report being barred from Instagram, and 7.9 percent say they are banished from Twitter, while 1.3 percent report being deplatformed from both.\n",
      "\n",
      "Centro analyzed the monetary impact of these bans and found that an influencer earning $4,000 a month could see their income drop 30 percent to $2,600 a month. Six months later, they would gain only $1,000 a month without social media. They could lose $30,000 a year.\n",
      "\n",
      "I understand why losing social media impacts bank accounts. For sex workers, social media is our central advertising platform. Kicking us off socials is like banning a movie studio from taking out billboard, print, and television ads. We lose our ability to sell our content. Companies hire me because they know that fans will click the link and press buy when I link out to my videos. In contracts, they include clauses requiring performers to post about their content on social media. I am a porn star because millions of people follow me, and I can market directly to them. Without my followers, I’m just another girl taking it all off on camera.\n",
      "\n",
      "Today’s porn market has undergone a massive paradigm shift from the ’70s porn industry, when suggestive posters turned Linda Boreman into Linda Lovelace, or the ’90s when VHS covers in adult video stores transformed Jenna Jameson into a household name. Today’s stars market porn studios’ content, not the other way around. Of course, when I appear in a significant companies’ porno, I boost my following, increasing my star stature and ensuring more studios hire me in the future. Most importantly, I come onto the radar of men or women watching streamers’ porn, and they then consider subscribing to my OnlyFans.\n",
      "\n",
      "Without social media, none of this would happen because I would lose the ability to market products. My product is myself. Without advertising myself on social media, I have no way to gain enough eyes for my products, and without a decent follower count, I’m less “valuable” to companies that might want to book me. And without a huge pool of people seeing my advertising every day, I have zero chance of selling my OnlyFans subscriptions. It’s a vicious cycle, and when a social media platform kicks me off, I fall off the merry-go-round.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "I have no idea how a performer could break into porn today without social media accounts. But it’s hard to get social media companies to care about sex workers the same way DeSantis has sounded the alarm on the consequences of deplatforming a controversial politician. Part of the problem is that sex workers operate on the periphery of mainstream society. I know a girl who rappers have offered to fuck in exchange for writing a song about her, but mainstream magazines have yet to profile her. Everyone watches porn, but porn stars hover on fringes. Nobody cares about us.\n",
      "\n",
      "Right now, I doubt Silicon Valley will reconsider its sex-worker policies. The media and public rarely cry out about porn stars losing their rights to post, so why would they? For now, I’ll be adding “former presidential candidate” to my Twitter bio. We’ll see if DeSantis and his ilk care the next time a social media giant deplatforms me, or if they only care when wacko politicians lose Twitter access for tweeting something crazy.\n"
     ]
    }
   ],
   "source": [
    "idx = 302\n",
    "print(df_test.loc[302].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "102f9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full text florida governor ron desantis announce week would fine social media company ban political candidates every outlet fox news msnbc fire missives bill get lose news coverage silicon valley deplatforms politicians save shockjocks like donald trump laura loomer want call politician say sex workers month centro university release study estimate percent adult influencers report lose access twitter instagram last year ban put permanent dent star income centro estimate sex workers lose million year due social media ban hear desantis fox news glenn greenwald socalled free speech warriors decry porn star lose incomes let break social media company screw porn star screw good way silicon valley titans revoke social media access multiple time take recent snapchat ban santa monicabased app bar post public account lose mean communicate fan would message snap lose percent revenue build back follow adult performers face far worse centro report show percent adult influencers report temporarily lose instagram account percent say briefly remove twitter percent claim suspend many performers regain access account everyone approximately percent performers report bar instagram percent say banish twitter percent report deplatformed centro analyze monetary impact ban find influencer earn month could see income drop percent month six months later would gain month without social media could lose year understand lose social media impact bank account sex workers social media central advertise platform kick us socials like ban movie studio take billboard print television ads lose ability sell content company hire know fan click link press buy link videos contract include clauses require performers post content social media porn star millions people follow market directly without followers another girl take camera today porn market undergo massive paradigm shift porn industry suggestive posters turn linda boreman linda lovelace vhs cover adult video store transform jenna jameson household name today star market porn studios content way around course appear significant company porno boost follow increase star stature ensure studios hire future importantly come onto radar men women watch streamers porn consider subscribe onlyfans without social media none would happen would lose ability market products product without advertise social media way gain enough eye products without decent follower count less valuable company might want book without huge pool people see advertise every day zero chance sell onlyfans subscriptions vicious cycle social media platform kick fall merrygoround advertisement idea performer could break porn today without social media account hard get social media company care sex workers way desantis sound alarm consequences deplatforming controversial politician part problem sex workers operate periphery mainstream society know girl rappers offer fuck exchange write song mainstream magazines yet profile everyone watch porn porn star hover fringe nobody care us right doubt silicon valley reconsider sexworker policies media public rarely cry porn star lose right post would add former presidential candidate twitter bio see desantis ilk care next time social media giant deplatforms care wacko politicians lose twitter access tweet something crazy\n"
     ]
    }
   ],
   "source": [
    "print(data.loc[302])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df49c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead36775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
